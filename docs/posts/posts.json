[
  {
    "path": "posts/GWalkR/",
    "title": "GWalkR_rclub",
    "description": "GWalkR",
    "author": [
      {
        "name": "Liz Hamel",
        "url": {}
      }
    ],
    "date": "2024-09-20",
    "categories": [],
    "contents": "\r\nGwalkR is a Tableau-like package that allows you to visualize and compare data without the need for ggplot and long code paragraphs.\r\nPlus it’s free!\r\nIt is simple and only requires 2 lines of code. Though, you do need to process and preformat any data you would like to use. This package allows you to visualize data and export graphs.\r\nIf you would like further information, you can visit the GitHub page https://github.com/Kanaries/GWalkR\r\nFirst Install the GWalkR package and load it\r\n\r\n\r\n# install.packages(\"GWalkR\")\r\nlibrary(GWalkR)\r\n\r\n\r\n\r\n\r\ndata(iris)\r\ngwalkr(iris)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-09-29T17:03:46-04:00",
    "input_file": "GWalkR-R-club-2024-presentation.knit.md"
  },
  {
    "path": "posts/2024-09-13-llms-in-r/",
    "title": "LLMs in R",
    "description": "Using LLMs in R",
    "author": [
      {
        "name": "Joel Mainland",
        "url": {}
      }
    ],
    "date": "2024-09-15",
    "categories": [],
    "contents": "\n\nContents\nUsing LLMs in R\nIntroduction\nGitHub Copilot\nExample 1: suggest package\nExample 2: continue pattern based on names of files\nExample 3: Ask question in comments\n\nChattr package\nExample 1: Create a simple chatbot in the Viewer\n\nAPI call to process text\nCompare to a hand-corrected version\n\n\n\nUsing LLMs in R\nIntroduction\nLet’s talk about a couple methods for using LLMs in R. I’ll be using three vignettes:\n1. GitHub Copilot\n2. chattr package\n3. API call to process text\nGitHub Copilot\nGitHub Copilot is an AI pair programmer that helps you write code faster. It is powered by OpenAI’s Codex, which is a language model trained on a diverse range of text, including code. Copilot can generate whole functions, suggest completions, and even write comments for you.\nHere is a short youtube video that can help you get Copilot and chattr working:\nhttps://www.youtube.com/watch?v=t7NrkAeosog\nExample 1: suggest package\n\n\nhide\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, readxl)\n\n#read in data\n\nFL_K_raw <- read_excel(\"data/raw/FL_KSU.xlsx\", sheet=3) # %>% clean_names()\n\n\nThis code throws an error because the clean_names() function is not loaded. Let’s see if Copilot can suggest the package for us.\n\n\nhide\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, readxl, janitor)\n\n#read in data\n\nFL_K_raw <- read_excel(\"data/raw/FL_KSU.xlsx\", sheet=3) %>% clean_names()\n\n\nAdd a comma after “readxl” and Copilot will suggest the janitor package.\nExample 2: continue pattern based on names of files\n\n\nhide\n\nFL_Ma <- read_excel(\"/Users/jmainland/Mainland Lab Dropbox/Joel Mainland/AROMA methods paper/data/raw/FL_monell_A.xlsx\", sheet = 3) \nFL_Mb <- read_excel(\"/Users/jmainland/Mainland Lab Dropbox/Joel Mainland/AROMA methods paper/data/raw/FL_monell_B.xlsx\", sheet = 3) \n\n\nThis code reads in two files, but we want to read in five files. Let’s see if Copilot can help us continue the pattern\nExample 3: Ask question in comments\n\n\nhide\n\n# q: What package can make radar plots?\n# a\n\n\n# regex to match the phone number in the format (xxx) xxx-xxxx\n\n\n#perform PCA on descriptors and make a plot using tidyverse\n\n\nThis code asks questions in the comments. Let’s see if Copilot can suggest a package that can make radar plots or regex to match a pattern.\nChattr package\nThe chattr package is a package that allows you to create interactive chatbots in R. It is built on top of the shiny package and provides a simple interface to create chatbots that can interact with users.\nExample 1: Create a simple chatbot in the Viewer\n\n\nhide\n\nremotes::install_github(\"mlverse/chattr\")\n#go to https://openai.com/blog/openai-api and sign in\n#personal > View API keys > Create a new secret key\n#Sys.setenv(OPENAI_API_KEY = “sk-…”)\n\n\n\nlibrary(chattr)\n#chattr_use(\"gpt35\")\n#chattr_use(\"gpt4\")\nchattr_use(\"copilot\")\n\n#chattr_test()\n\n#chattr_app(as_job = TRUE)\n\n\nYou can run a window inside the R interface, but it doesn’t know about your code or data frames, so to me is just a worse interface than pasting into the webpage.\nOne benefit is that you can use a local model to keep data private\nAPI call to process text\n\n\nhide\n\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(stringr)\n\napi_key <- Sys.getenv(\"OPENAI_API_KEY\")\n# Calls the ChatGPT API with the given prompt and returns the answer\nask_chatgpt <- function(prompt) {\n  response <- POST(\n    url = \"https://api.openai.com/v1/chat/completions\", \n    add_headers(Authorization = paste(\"Bearer\", api_key)),\n    content_type_json(),\n    encode = \"json\",\n    body = list(\n      model = \"gpt-3.5-turbo\",\n      messages = list(list(\n        role = \"user\", \n        content = prompt\n      ))\n    )\n  )\n  str_trim(content(response)$choices[[1]]$message$content)\n}\n\ncorrect_spelling <- function(text) {\n  prompt = paste(\"Correct the spelling of the following word, only returning the corrected word itself. All words refer to smells, so correct to a word that might be used to describe an odor or desribe a chemical:\", text)\n  corrected_text <- ask_chatgpt(prompt)\n  return(corrected_text)\n}\n\n#Now check on a df\ndf <- tibble(\n  words = c(\"speling\", \"korrect\", \"writting\", \"exmaple\",\"menty\")\n)\n\ndf_corrected <- df %>%\n  mutate(corrected_words = map_chr(words, correct_spelling))\n\nhead(df_corrected)\n\n# A tibble: 5 × 2\n  words    corrected_words\n  <chr>    <chr>          \n1 speling  smelling       \n2 korrect  correct        \n3 writting whiffing       \n4 exmaple  example        \n5 menty    minty          \n\nCompare to a hand-corrected version\n\n\nhide\n\nEJM_spellcheck <- data_frame(word = c(\"perfumy\", \"mustyearthy\", \"menty\", \"buteric\", \"sulpher\", \"cinnamons\", \"vanallyn\", \n                                      \"chemcal\", \"animatic\", \"butric\", \"chloine\", \"chorine\", \"searmint\", \"medicnal\", \"tanic\", \n                                      \"grren\", \"spicey\", \"carmel\", \"diasiteal\", \"carmalized\", \"aromatica\", \"antaseptic\",\n                                      \"planty\", \"greeny\", \"alchohol\", \"pwder\", \"friuty\", \"sweey\", \"greenage\", \"acidiic\", \"saopy\",\n                                      \"liqour\", \"mente\", \"anaseed\", \"chrismas\", \"seasfood\", \"strng\", \"unplesent\", \"sublte\", \"moutwash\",\n                                      \"wintogreen\", \"fruty\", \"parfume\", \"vegatable\", \"overriped\", \"citronellaish\", \"sauekruat\", \"oinion\",\n                                      \"pungant\", \"cinammon\", \"unplesant\", \"anticeptic\"),\n                             EJMfix = c(\"perfume\", \"musty earthy\", \"mint\", \"butyric\", \"sulphur\", \"cinnamon\", \"vanillin\", \n                                     \"chemical\", \"animalic\", \"butyric\", \"chlorine\", \"chlorine\", \"spearmint\", \"medicinal\", \"tannic\", \n                                     \"green\", \"spicy\", \"caramel\", \"diacetyl\", \"caramelized\", \"aromatic\", \"antiseptic\",\n                                     \"plant\", \"green\", \"alcohol\", \"powder\", \"fruity\", \"sweet\", \"green\", \"acidic\", \"soapy\",\n                                     \"liquor\", \"mint\", \"anise\", \"christmas\", \"seafood\", \"strong\", \"unpleasent\", \"subtle\", \"mouhtwash\",\n                                     \"wintergreen\", \"fruity\", \"perfume\", \"vegetable\", \"overripe\", \"citronella\", \"sauerkraut\", \"onion\",\n                                     \"pungent\", \"cinnamon\", \"unpleasant\", \"antiseptic\"))\n\n#Note that putting your API calls in loops can be a bad idea, since you are spending money on each call. That said, the API is pretty cheap.\n\n# https://platform.openai.com/usage\n\ndf_corrected2 <- EJM_spellcheck %>%\n  mutate(corrected_by_LLM = map_chr(word, correct_spelling))\n\nhead(df_corrected2)\n\n# A tibble: 6 × 3\n  word        EJMfix       corrected_by_LLM\n  <chr>       <chr>        <chr>           \n1 perfumy     perfume      perfumey        \n2 mustyearthy musty earthy musty           \n3 menty       mint         minty           \n4 buteric     butyric      butyric         \n5 sulpher     sulphur      sulfur          \n6 cinnamons   cinnamon     cinnamal        \n\n\n\n\n",
    "preview": {},
    "last_modified": "2024-09-15T05:55:47-04:00",
    "input_file": "llms-in-r.knit.md"
  },
  {
    "path": "posts/DREAM Olfactory Mixtures 1.0/",
    "title": "DREAM Mixture ML Intro",
    "description": {},
    "author": [
      {
        "name": "Joel Mainland",
        "url": {}
      }
    ],
    "date": "2024-06-15",
    "categories": [],
    "contents": "\n\nContents\nDREAM Olfactory Mixtures Challenge\nLook at the data\nVisualize the data\nBasic machine learning\nSplit the data\nFit a linear model\nMake predictions\nCalculate metrics\nRandom Forest\nTry the random forest model on the test set\nLook at which variables are most important\n\n\n\nDREAM Olfactory Mixtures Challenge\nThe DREAM Olfactory Mixtures Challenge launched in April of 2024.\nIn this post we will take a look at the training data for this challenge and do some very basic machine learning.\nRecent advances in predictive methods and availability of perceptual data have paved the way for a growing interest in olfactory perception predictions from chemical representations of molecules. This has led to a growing consensus that for pure odors, it is possible to build models using the chemical structure of molecules to predict the perceptual values of natural language attributes of smells. However, predictions have mainly focused on pure molecules and not the real-world situation of olfactory mixtures. In order to start filling this gap, we plan to organize a second DREAM olfaction prediction challenge now focused on predicting the discriminability of olfactory mixtures. Using publicly available data from 3 different studies (Bushdid et al 2014, Snitz et al 2013, Ravia et al 2020) for more than 700 unique mixtures and almost 600 measurements of mixture pairs discriminability, participants will be tasked to predict the discriminability of 46 unpublished mixture pairs.\nYou can register for the challenge here: https://www.synapse.org/Synapse:syn53470621/wiki/626022\nLook at the data\n\n\nhide\n\n#This is generated from Preprocessing.R\nmixturesWithFeatures <- read.csv(\"data/processed/MixturesWithFeatures.csv\")\n\n# You can load from Dropbox if you are having issues loading from the workspace:\n# mixturesWithFeatures <- read.csv(\"https://www.dropbox.com/scl/fi/f75j07xedtsv3774con0k/MixturesWithFeatures.csv?rlkey=lyfw541vdb6byhpj1341etvnu&dl=1\", row.names=NULL)\n\nglimpse(mixturesWithFeatures)\n\nRows: 548\nColumns: 11\n$ dataset             <chr> \"Snitz 1\", \"Snitz 1\", \"Snitz 1\", \"Snitz …\n$ mixture_1           <int> 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 8, 8, 8, 8…\n$ mixture_2           <int> 2, 3, 4, 5, 6, 7, 38, 39, 40, 41, 2, 3, …\n$ experimental_values <dbl> 0.6041667, 0.6510417, 0.5260417, 0.50520…\n$ a                   <chr> \"6501;264;2879;7685;7731;326;7888;61138;…\n$ b                   <chr> \"240;93009;323;8148;7762;3314;460;6184;7…\n$ num_compound_a      <int> 10, 10, 10, 10, 10, 10, 1, 1, 1, 1, 1, 1…\n$ num_compound_b      <int> 10, 1, 20, 30, 40, 4, 15, 20, 30, 43, 10…\n$ diff_mixture_size   <int> 0, 9, 10, 20, 30, 6, 14, 19, 29, 42, 9, …\n$ overlap_percent     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ angle_dist          <dbl> 0.03592652, 0.04259934, 0.01889634, 0.02…\n\nVisualize the data\n\n\nhide\n\nmixturesWithFeatures %>%\n  ggplot(aes(x = experimental_values)) +\n  geom_histogram() +\n  labs(x=\"Percentage discrimination\")\n\n\n\nHigh values on the x-axis correspond to mixture pairs that are easy to discriminate. There are a handful of mixture pairs that are very similar to each other.\n\n\nhide\n\nggpairs(mixturesWithFeatures,columns=c(4,9,10,11))\n\n\n\n\n\nhide\n\n#Discrimination vs. Difference in size\nmixturesWithFeatures %>%\n  ggplot(aes(y = diff_mixture_size, x = experimental_values)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  labs(x=\"Percentage discrimination\",y=\"Difference in size\")\n\n\n\nIn general, mixture pairs that differ in the number of components are easier to discriminate. This effect appears to be driven by the very similar pairs that are all also very similar in size.\n\n\nhide\n\n#Discrimination vs. Percentage overlap\nmixturesWithFeatures %>%\n  ggplot(aes(y = overlap_percent, x = experimental_values)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  labs(x=\"Percentage discrimination\",y=\"Percentage overlap\")\n\n\n\nIn general, pairs with low overlap in components are easy to discriminate.\nWe can see one mixture pair that has no overlap in molecules, but could only be discriminated on ~15% of trials. This is a possible metameric pair.\nSnitz et al., 2013 has a published algorithm for predicting how similar two mixtures are, and their data are in this training set. Let’s see how well that model does on all of the data.\n\n\nhide\n\n#Discrimination vs. Angle Distance\nmixturesWithFeatures %>%\n  ggplot(aes(y = angle_dist, x = experimental_values)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  labs(x=\"Percentage discrimination\",y=\"Angle Distance\")\n\n\nhide\n\nfit1 <- lm(angle_dist ~ experimental_values, data = mixturesWithFeatures)\n\nprint(paste0(\"Adj R2 = \",signif(summary(fit1)$adj.r.squared, 5)))\n\n[1] \"Adj R2 = 0.12849\"\n\nhide\n\nprint(paste0(\" P =\",signif(summary(fit1)$coef[2,4], 5)))\n\n[1] \" P =2.805e-18\"\n\nBasic machine learning\nSplit the data\n\n\nhide\n\nmixtures.clean <- mixturesWithFeatures %>% \n  #mutate(ID = row_number()) %>%\n  select(experimental_values,diff_mixture_size,overlap_percent,angle_dist)\n\n#divide into training and test sets\nset.seed(42)\n# Create a data partition: 80% for training, 20% for testing\ntrainIndex <- createDataPartition(mixtures.clean$experimental_values, p = 0.8, list = FALSE)\n\n# Create the training and testing sets\ntrain_set <- mixtures.clean[trainIndex, ]\ntest_set <- mixtures.clean[-trainIndex, ]\n\n\nFit a linear model\n\n\nhide\n\n#fit a linear model\nmodel_linear <- lm(experimental_values ~ diff_mixture_size + overlap_percent + angle_dist, data = train_set)\n\n# View the summary of the model\nsummary(model_linear)\n\n\nCall:\nlm(formula = experimental_values ~ diff_mixture_size + overlap_percent + \n    angle_dist, data = train_set)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46122 -0.09269  0.00589  0.09136  0.35990 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        0.5705318  0.0178630  31.939  < 2e-16 ***\ndiff_mixture_size  0.0003972  0.0007227   0.550 0.582810    \noverlap_percent   -0.0020038  0.0002810  -7.131 4.17e-12 ***\nangle_dist         1.0574525  0.3005934   3.518 0.000481 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1349 on 437 degrees of freedom\nMultiple R-squared:  0.2558,    Adjusted R-squared:  0.2507 \nF-statistic: 50.06 on 3 and 437 DF,  p-value: < 2.2e-16\n\nMake predictions\n\n\nhide\n\ntest_set$predicted <- predict(model_linear, newdata = test_set)\n\ntest_set %>% \n  ggplot(aes(x=experimental_values,y=predicted))+\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  labs(x=\"Percentage discrimination\",y=\"Predicted\")\n\n\n\nCalculate metrics\n\n\nhide\n\n# Calculate Mean Squared Error\nmse <- mean((test_set$experimental_values - test_set$predicted)^2)\nprint(paste(\"Mean Squared Error:\", mse))\n\n[1] \"Mean Squared Error: 0.0191725597427043\"\n\nhide\n\n# Calculate R-squared on the test set\nss_total <- sum((test_set$experimental_values - mean(test_set$experimental_values))^2)\nss_residual <- sum((test_set$experimental_values - test_set$predicted)^2)\nr_squared <- 1 - (ss_residual / ss_total)\nprint(paste(\"R-squared on test set:\", r_squared))\n\n[1] \"R-squared on test set: 0.263343178822379\"\n\nRandom Forest\n\n\nhide\n\n#Build a random forest\nr = randomForest(experimental_values ~., data=train_set, importance=TRUE, do.trace=100)\n\n     |      Out-of-bag   |\nTree |      MSE  %Var(y) |\n 100 |  0.01827    75.39 |\n 200 |  0.01816    74.93 |\n 300 |  0.01798    74.20 |\n 400 |  0.01797    74.16 |\n 500 |  0.01798    74.18 |\n\nhide\n\nprint(r)\n\n\nCall:\n randomForest(formula = experimental_values ~ ., data = train_set,      importance = TRUE, do.trace = 100) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.01797761\n                    % Var explained: 25.82\n\nTree shows the number of trees at each stage of evaluation.\nMSE is the mean-squared error of the predictions for out-of-bag samples.\nPercentage of variance explained–higher is better.\nMSE decreases slightly as we increase the number of trees, but variance explained slightly decreases. The model stablilizes around 300 trees.\nNote that the final variance explained is much lower than the estimates. This is likely because we are overfitting with only three variables.\nTry the random forest model on the test set\n\n\nhide\n\n#Now try it on the test set\nmixture.predict = predict(r, test_set)\nmixture.results <- cbind(test_set,Predicted=mixture.predict)\n\nmixture.results %>%\n  ggplot(aes(y = mixture.predict, x = experimental_values)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  labs(x=\"Percentage discrimination\",y=\"Random Forest Model\")\n\n\nhide\n\nfit2 <- lm(mixture.predict ~ experimental_values, data = mixture.results)\n\nprint(paste0(\"Adj R2 = \",signif(summary(fit2)$adj.r.squared, 5)))\n\n[1] \"Adj R2 = 0.3068\"\n\nhide\n\nprint(paste0(\" P =\",signif(summary(fit2)$coef[2,4], 5)))\n\n[1] \" P =3.6527e-10\"\n\nLook at which variables are most important\n\n\nhide\n\nimportance(r)\n\n                   %IncMSE IncNodePurity\ndiff_mixture_size 13.42197      1.115440\noverlap_percent   35.78884      2.565605\nangle_dist        30.76258      3.001925\n\n%IncMSE: This column shows the percentage increase in the mean squared error (MSE) when a given variable is randomly permuted (its values are shuffled). A higher value indicates that the variable is more important for predicting the target variable because permuting it leads to a larger increase in the MSE.\nIncNodePurity: This column shows the total decrease in node impurity (measured by residual sum of squares for regression) that results from splits on this variable, averaged over all trees. A higher value indicates that the variable contributes more to reducing the impurity of nodes in the trees, thus making it a more important predictor.\n\n\nhide\n\nvarImpPlot(r)\n\n\n\nDifference in mixture size is the least important variable. The two metrics disagree on which of the other two predictors are more important\n\n\n\n",
    "preview": "posts/DREAM Olfactory Mixtures 1.0/ML-Intro_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2024-06-15T12:01:28-04:00",
    "input_file": "ML-Intro.knit.md"
  },
  {
    "path": "posts/2024-05-16-uploading-to-the-r-club-blog1/",
    "title": "Uploading to the R club Blog",
    "description": "R Club on May 17th 2024: How to upload to the blog",
    "author": [
      {
        "name": "Vince Ramirez",
        "url": {}
      }
    ],
    "date": "2024-05-17",
    "categories": [],
    "contents": "\nA note: I am still learning the distill package and how distill blogs work. The current tutorial is to demonstrate the current workflow I have, and to motivate any R clubbers that want to help to maintain this blog.\nPurpose\nTo demonstrate how you can start contributing to the R club Blog and archive past R club presentations.\nWho?\nAnyone that takes part in R club.\nGetting Started\nIf you do not already use github/git, then please familiarize yourself with the basics. There are a number of GUI desktop applications that make using github very easy. Unfortunately I will, not cover using github in depth during today’s tutorial.\nTo contribute to the repository you will need to clone the github repository for the blog, and work within the project’s directories.\nI am assuming you are using R studio. To make everything run smoothly you should make Rmarkdown documents the standard for your R club presentations.\nFirst we will need to install the distill package.\ninstall.packages(\"distill\")\nThis will install the distill package which will allow your work to be formatted in a manner that can be implemented into the blog repository.\nThe easiest way to create a new blog post is to enter the following command in the directory of the blog\ndistill::create_post() \nThis will create a new post in the /_posts/ directory of the git repository.\ni created a new post for this R club (see below):\n\nYou can then add in your Rmarkdown file, or create a new Rmarkdown file into this repository.\nWhen beginning a new Rmarkdown file that you intend to have as a blog post, you should use the distill template.\n\nBe sure to format your YAML appropriately. An example is below\n\nYou can then run your Rmarkdown as usual.\nExample\nThe following data and example is based on analysis found at https://github.com/davestroud/BeerStudyhttps://github.com/davestroud/BeerStudy\nThe authors of this data set were interested in looking at the relationship between alcohol content (alcohol by volume or ABV) and bitterness of beer (IBU).\n\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(magrittr)\nlibrary(ggplot2)\nBeers <- read_csv(\"data/Beers.csv\")\nBreweries <- read_csv(\"data/Breweries.csv\")\n\nBeers <- Beers %>%\n  rename(Brew_ID = \"Brewery_id\")\n\nBeer_data <- left_join(Beers,Breweries, by = \"Brew_ID\")\n\n\nFirst we can look at the distribution of IBU (International Bittering Units).\n\n\nBeer_data %>%\n  ggplot(aes(x = IBU)) +\n  geom_histogram()\n\n\n\nMost beers fall under 100 IBUs, although some do approach 120+. Many beers do not have IBUs listed.\nWe can also look at ABV (alcohol by volume)\n\n\nBeer_data %>%\n  ggplot(aes(x = ABV)) +\n  geom_histogram()\n\n\n\nMost beers are under 10% ABV.\nHow does ABV and IBU correlate?\n\n\nBeer_data %>%\n  ggplot(aes(x = IBU, y = ABV)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\nBeer_data %>%\n  ggplot(aes(x = IBU, y = ABV)) +\n  geom_point() +\n  geom_smooth(method = \"loess\")\n\n\n\nWhich State has the highest median ABV?\n\n\nMedian_ABV_IBU_State <- Beer_data %>%\n  na.omit() %>%\n  group_by(State) %>% \n  summarise(`Median ABV` = median(ABV, na.rm = T),\n            `Median IBU` = median(IBU, na.rm = T)\n            )\n\n\nMedian_ABV_IBU_State %>%\n  ggplot(aes(x = reorder(State, `Median ABV`), y = `Median ABV`)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(size=rel(0.85), angle=90)) +\n  xlab(\"State\")\n\n\n\nWhich state has the highest median IBUs?\n\n\nMedian_ABV_IBU_State %>%\n  ggplot(aes(x = reorder(State,`Median IBU`),y = `Median IBU`)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(size = rel(0.85), angle = 90)) +\n  xlab(\"State\")\n\n\n\nLet’s Plot it on a Map\nHere I plot which state has the highest median ABV using.\n\n\nlibrary(usmap)\n\nMedian_ABV_IBU_State_ <- Median_ABV_IBU_State %>%\n  rename(state = \"State\")\n\nplot_usmap(data=Median_ABV_IBU_State_,values = \"Median ABV\") +\n  scale_fill_gradient2(low = \"red\",\n                       mid = \"white\",\n                       high = \"blue\",\n                       midpoint = 0.053)\n\n\n\nAfter You Finish You Should Knit The Distill Article\nThis will create an html in our _post directory, and your post will be updated in the /docs/posts/ directory.\n\n\nChecking Your Distill HTML “index.html”\n\nFinal Steps\nYou need to push to github. We will need to figure out a work flow to have individuals push to the master branch when they have finished putting together their R markdown files. More to come soon.\n\n\n\n",
    "preview": "posts/2024-05-16-uploading-to-the-r-club-blog1/Example_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2024-05-17T12:30:10-07:00",
    "input_file": "Example.knit.md"
  },
  {
    "path": "posts/R Club November 12/",
    "title": "Getting Started with ggplot2!",
    "description": "R-Club November 12 2021",
    "author": [
      {
        "name": "Vince Ramirez",
        "url": {}
      }
    ],
    "date": "2021-11-12",
    "categories": [],
    "contents": "\nIntroduction\nThe goal of today is to learn the basic structure of a ggplot2 command to create graphics. We will be working with the mtcars dataset which is already loaded into R.\nA quick view of this data can be done using the glimpse, str, or head functions. Take your pick!\n\n\nstr(mtcars)\n\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nglimpse(mtcars)\n\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1…\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4…\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,…\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180…\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3…\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190,…\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,…\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1…\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1…\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4…\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2…\n\nPlotting Using base-R\nWe can use the base functions in R to make plots of these data. R itself has very powerful graphic capabilities, and for many this works just fine.\nA simple plot may want to look at the relationship between horsepower and miles per gallon. This is easy visualized below\n\n\nplot(hp~mpg,data = mtcars,ylab=\"horsepower\",xlab=\"miles per gallon\")\n\n\n\n\nAn alternative method is to use the package ggplot2 in order to create your visuals.\nggplot2: A primer\nggplot2 is a powerful graphics library in R which is part of the larger tidyverse. It is praised for the ability to create powerful and highly customizable visuals with relatively simple commands.\nThese commands follow a very generic structure which is defined as follows:\nggplot2(data= x, mappings=aes())+ geom_function()\nor\nggplot2(data) + geom_function(mapping=aes())\nWhere aes() is the aesthetics in your ggplot. Here we will define our X and Y variables as well as our aesthetic choices such as our coloring schemes.\nA simple ggplot is shown below.\n\n\nggplot(data=mtcars, aes(x=mpg, y=hp)) +\n  geom_point()\n\n\n\n\nWe can see that we have recreated the plot made using base-R.\nLet’s take it a step further and customize our plot. We can invoke themes\n\n\nggplot(data=mtcars, aes(x=mpg, y=hp)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nWe can change our axis-labels\n\n\nggplot(data=mtcars, aes(x=mpg, y=hp)) + geom_point() + theme_bw() + labs(x=\"Miles Per Gallon\", y=\"Horsepower\")\n\n\n\n\nWe can change our aesthetics to categorize our data based on the number of cylinders in the engine. It needs to be converted to a factor variable (categorical variable) first.\n\n\nggplot(data=mtcars, aes(x=mpg, y=hp,col=as.factor(cyl))) +\n  geom_point() +\n  theme_bw() +\n  labs(x=\"Miles Per Gallon\", y=\"Horsepower\")\n\n\n\n\nWe can adjust our legend with the labs() command as well.\n\n\nggplot(data=mtcars, aes(x=mpg, y=hp,col=as.factor(cyl))) +\n  geom_point() +\n  theme_bw() +\n  labs(col=\"Cylinders\",x=\"Miles Per Gallon\", y=\"Horsepower\")\n\n\n\n\nIf we want a different graph for each engine type we can use facets.\n\n\nggplot(data=mtcars, aes(x=mpg, y=hp)) +\n  geom_point() +\n  theme_bw() +\n  labs(x=\"Miles Per Gallon\", y=\"Horsepower\") + facet_wrap(~cyl)\n\n\n\n\nThis just scratches the surface of what ggplot is capable of. It is a simple example, but the possibilities are almost endless.\nDifferent Graphic Types\nI have displayed a simple x-y scatter plot, but the process for other plot types is very similar. Here is an example of a highly customized plot. I will use a similar set of data already loaded into R called “mpg”.\n\n\nstr(mpg)\n\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nI will create a violin plot which shows the distribution of the data. I am interested in mile per gallon in the city and the number of cylinders in the engine.\n\n\nggplot(data=mpg, aes(x=as.factor(cyl),y=cty)) +\n  geom_violin() +\n  theme_bw() +\n  labs(x=\"Number of Cylinders in Engine\", y=\"Miles per Gallon in the City\")\n\n\n\n\nThis reveals a lot already, but we can add more. Let’s add a box and whisker to better understand the distribution.\n\n\nggplot(data=mpg, aes(x=as.factor(cyl),y=cty)) +\n  geom_violin() +\n  theme_bw() + \n  geom_boxplot(width=0.1) +\n  labs(x=\"Number of Cylinders in Engine\", y=\"Miles per Gallon in the City\")\n\n\n\n\nWe can also add the individual points to this graph. I want to jitter the points to prevent overplotting and make them semi-transparent.\n\n\nggplot(data=mpg, aes(x=as.factor(cyl),y=cty)) +\n  geom_violin() +\n  theme_bw() + \n  geom_boxplot(width=0.1) +\n  geom_jitter(alpha=0.4) +\n  labs(x=\"Number of Cylinders in Engine\", y=\"Miles per Gallon in the City\")\n\n\n\n\nNext I will adjust the width of our jittered points.\n\n\nggplot(data=mpg, aes(x=as.factor(cyl),y=cty)) +\n  geom_violin() +\n  theme_bw() + \n  geom_boxplot(width=0.1) +\n  geom_jitter(alpha=0.3, width=0.2) +\n  labs(x=\"Number of Cylinders in Engine\", y=\"Miles per Gallon in the City\")\n\n\n\n\nWrapping Up\nggplot2 allows us to create powerful and customizable graphics using relatively simple commands. The library is well documented and maintained allowing R novices and experts to quickly pick it up. We have barely scratched the surface of what ggplot2 is able to do. Additionally, user written companion libraries exist which extend the functionality of ggplot2. The possibilities are endless.\n\n\n\n",
    "preview": "posts/R Club November 12/Getting-Started-with-ggplot2_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-11-18T09:36:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Rclub Blog",
    "description": "Welcome to the Rclub Blog.",
    "author": [
      {
        "name": "Vince Ramirez",
        "url": {}
      }
    ],
    "date": "2021-11-06",
    "categories": [],
    "contents": "\nIntroducing the R club Blog! A space for us to share everything R!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-18T09:36:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/Writing Function R Club October 11/",
    "title": "R Club - Writing Functions",
    "description": "R Club",
    "author": [],
    "date": "2021-11-06",
    "categories": [],
    "contents": "\nIntroduction\nWords from our sponsors\n\n“Writing good functions is a lifetime journey. Even after using R for many years I still learn new techniques and better ways of approaching old problems.” -Hadley Wickham\n\nOne of the best ways to improve your reach as a data scientist is to write functions. Functions allow you to automate common tasks in a more powerful and general way than copy-and-pasting. Writing a function has three big advantages over using copy-and-paste:\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\nOther material\nSource Material for Functions\nhttps://r4ds.had.co.nz/functions.html ## Source Material on Purrr https://dcl-prog.stanford.edu/purrr-parallel.html ## More on Purrr https://r4ds.had.co.nz/iteration.html#the-map-functions ## Purrr Cheatsheat https://github.com/rstudio/cheatsheets/blob/master/purrr.pdf\nCode\nPackages / Functions\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# R Version \nR.Version()$version.string #code ran on R version 4.0.5 (2021-03-31)\n\n\n[1] \"R version 4.1.0 (2021-05-18)\"\n\n# Load packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(magrittr, scales, tidyverse)\n\nget_data <- function(){\n  df <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10))\n  \n  return(df)\n}\n\n\n\nData Read\n\n\n# Create a table of 10 random number with a mean of zero (SD=1)\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf\n\n\n# A tibble: 10 x 4\n        a       b      c      d\n    <dbl>   <dbl>  <dbl>  <dbl>\n 1  0.973 -0.0730  0.823 -0.151\n 2  0.953 -0.282  -0.729  0.967\n 3  0.425  0.962  -0.611 -0.795\n 4  1.91   0.323  -0.194 -0.205\n 5 -0.741 -0.429  -0.370 -0.616\n 6 -0.245  0.269  -0.509 -0.110\n 7  1.42  -1.38   -0.407 -0.272\n 8  0.868  0.402  -0.148  0.156\n 9 -0.849  1.86    1.83  -1.25 \n10 -0.509  3.54    1.29  -0.378\n\nMaking a function\n\n\n# Rescale data to be between 0 and 1\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b <- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE)) #look a mistake!\ndf$c <- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d <- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\ndf # look at column B for the mistake from paste/copy\n\n\n# A tibble: 10 x 4\n        a     b      c     d\n    <dbl> <dbl>  <dbl> <dbl>\n 1 0.660  0.369 0.607  0.495\n 2 0.653  0.310 0      1    \n 3 0.462  0.661 0.0463 0.204\n 4 1      0.481 0.209  0.471\n 5 0.0394 0.268 0.140  0.285\n 6 0.219  0.466 0.0861 0.513\n 7 0.822  0     0.126  0.440\n 8 0.622  0.503 0.227  0.633\n 9 0      0.916 1      0    \n10 0.123  1.39  0.789  0.392\n\n# Reload OG dataframe\ndf <- get_data()\n\n# Ask yourself: Am I repeating code? Am I copy-pasting stuff?\n  ## YES! What am I repeating?\n  df$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n  \n  ## Reload OG dataframe\n  df <- get_data()\n\n  ## Rewrite the code using temporary variables with general names\n  x <- df$a\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n\n\n [1] 0.3798973 0.6395313 0.3916441 0.6788337 0.0000000 0.4547929\n [7] 1.0000000 0.8133605 0.5359254 0.3890551\n\n# Ask yourself: Is there still duplication?\n  ## YES! What am I repeating? [in this case, it's the range]\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n\n\n [1] 0.3798973 0.6395313 0.3916441 0.6788337 0.0000000 0.4547929\n [7] 1.0000000 0.8133605 0.5359254 0.3890551\n\n# Make the function\n  rescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n  }\n  \n  ## Check to make sure it works as expected\n  rescale01(c(0, 5, 10))\n\n\n[1] 0.0 0.5 1.0\n\n  ## Add to other customized functions and apply to dataframe\n  df$a <- rescale01(df$a)\n  df$b <- rescale01(df$b)\n  df$c <- rescale01(df$c)\n  df$d <- rescale01(df$d)\n\n\n\nThree things to making a function: 1. You need to pick a name for the function. Here I’ve used rescale01 because this function rescales a vector to lie between 0 and 1. + Function names should be verbs, and arguments should be nouns. + But if the function can be nouns if they represents known nouns (e.g. mean()) or properties of an object (e.g. coef()) + snake_case vs. camelCase - just be consistent + Avoid common function names as your local will have priority\nYou list the inputs, or arguments, to the function inside function. Here we have just one argument. If we had more the call would look like function(x, y, z).\nDefault value have a value set here. eg. rescale01(tmp, Finite = TRUE)\nThere are rules for some names: ++ x,y,z: vectors ++ df: dataframe ++ i,j: numberic indices (typically rows and columns respectively) ++ n: length, number of rows ++ p: number of columns\nYou place the code you have developed in body of the function, a { block that immediately follows function(…).\nstop() is useful to place in conditions if you only accept certain data; if(finite = FALSE){stop(“you can only have finite number here dude”)}\nreturn() another way to break out of a function and return a value\ninvisible() send an object, but don’t print it. Good for keeping things pipeable\nBetter functions\n\n\n# Reload OG dataframe\ndf <- get_data()\n\n# Make our rescale function even better - add default function argument and stop rule\nrescale01 <- function(x, finite=TRUE) {\n  # check for finite numbers in vector\n  if(finite == FALSE){\n    stop(\"you can only have finite number here dude!\")  \n  } \n  \n  #grab range (rng) to use min (rng[1]) and max (rng[2])\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n  \n}\n\n# Call function with attribute\nrescale01(df$a, finite = TRUE)\n\n\n [1] 0.4496844 0.5073375 0.4230537 1.0000000 0.7514802 0.5768766\n [7] 0.3203819 0.0000000 0.3222939 0.2888145\n\n# Pipe Dream - never break the pipe\nshow_missings <- function(df) {\n  n <- sum(is.na(df))\n  cat(\"Missing values: \", n, \"\\n\", sep = \"\")\n  \n  invisible(df)\n}\n\nshow_missings(mtcars) \n\n\nMissing values: 0\n\nmtcars %>% \n  show_missings() %>% \n  mutate(mpg = ifelse(mpg < 20, NA, mpg)) %>% \n  show_missings() \n\n\nMissing values: 0\nMissing values: 18\n\npurrrFect functions\nFaster as they are written in C\nRun multiple columns within the pipe\n\n\n# What if I want to run all cols with a function?\ndf %>%\nmap(rescale01)\n\n\n\n$a\n [1] 0.4496844 0.5073375 0.4230537 1.0000000 0.7514802 0.5768766\n [7] 0.3203819 0.0000000 0.3222939 0.2888145\n\n$b\n [1] 0.2238090 0.1229223 0.0000000 0.5674141 0.2684266 0.3186047\n [7] 1.0000000 0.5474187 0.5649674 0.1791412\n\n$c\n [1] 1.0000000 0.0000000 0.5075496 0.2040886 0.3651896 0.1734424\n [7] 0.1896919 0.2876244 0.6463605 0.4859685\n\n$d\n [1] 0.09331861 0.36398592 0.32631510 0.14670286 0.48385989 0.70930657\n [7] 1.00000000 0.05151067 0.12948621 0.00000000\n\ndf %>%\n  map_dbl(mean)\n\n\n         a          b          c          d \n 0.6881837  0.1634370 -0.2770932 -0.1907347 \n\n# But what about spitting out a vector instead of a list?\ndf %>%\n  map(rescale01) %>% str()\n\n\nList of 4\n $ a: num [1:10] 0.45 0.507 0.423 1 0.751 ...\n $ b: num [1:10] 0.224 0.123 0 0.567 0.268 ...\n $ c: num [1:10] 1 0 0.508 0.204 0.365 ...\n $ d: num [1:10] 0.0933 0.364 0.3263 0.1467 0.4839 ...\n\ndf %>%\n  modify(rescale01)\n\n\n# A tibble: 10 x 4\n       a     b     c      d\n   <dbl> <dbl> <dbl>  <dbl>\n 1 0.450 0.224 1     0.0933\n 2 0.507 0.123 0     0.364 \n 3 0.423 0     0.508 0.326 \n 4 1     0.567 0.204 0.147 \n 5 0.751 0.268 0.365 0.484 \n 6 0.577 0.319 0.173 0.709 \n 7 0.320 1     0.190 1     \n 8 0     0.547 0.288 0.0515\n 9 0.322 0.565 0.646 0.129 \n10 0.289 0.179 0.486 0     \n\n# What if I want to run a few cols with function?\ndf %>%\n  modify_at(c(\"a\", \"b\"), rescale01)\n\n\n# A tibble: 10 x 4\n       a     b      c      d\n   <dbl> <dbl>  <dbl>  <dbl>\n 1 0.450 0.224  2.18  -0.739\n 2 0.507 0.123 -1.82  -0.113\n 3 0.423 0      0.210 -0.200\n 4 1     0.567 -1.01  -0.616\n 5 0.751 0.268 -0.360  0.164\n 6 0.577 0.319 -1.13   0.685\n 7 0.320 1     -1.06   1.36 \n 8 0     0.547 -0.671 -0.836\n 9 0.322 0.565  0.765 -0.655\n10 0.289 0.179  0.123 -0.955\n\ndf %>%\n  mutate_at(c(\"a\", \"b\"), ~ rescale01(.))\n\n\n# A tibble: 10 x 4\n       a     b      c      d\n   <dbl> <dbl>  <dbl>  <dbl>\n 1 0.450 0.224  2.18  -0.739\n 2 0.507 0.123 -1.82  -0.113\n 3 0.423 0      0.210 -0.200\n 4 1     0.567 -1.01  -0.616\n 5 0.751 0.268 -0.360  0.164\n 6 0.577 0.319 -1.13   0.685\n 7 0.320 1     -1.06   1.36 \n 8 0     0.547 -0.671 -0.836\n 9 0.322 0.565  0.765 -0.655\n10 0.289 0.179  0.123 -0.955\n\ndf2 <- df\n\n\n# What if we're only going to run a function once? Answer: Anonymous functions\ndf %>%\n  modify(function(x) x+2)\n\n\n# A tibble: 10 x 4\n       a     b     c     d\n   <dbl> <dbl> <dbl> <dbl>\n 1  2.64  1.71 4.18   1.26\n 2  2.83  1.42 0.178  1.89\n 3  2.56  1.06 2.21   1.80\n 4  4.43  2.71 0.995  1.38\n 5  3.62  1.84 1.64   2.16\n 6  3.05  1.99 0.872  2.69\n 7  2.22  3.96 0.937  3.36\n 8  1.18  2.65 1.33   1.16\n 9  2.23  2.70 2.77   1.34\n10  2.12  1.58 2.12   1.05\n\n#shortcuts from purrr\ndf %>%\n  modify(~ . + 2)\n\n\n# A tibble: 10 x 4\n       a     b     c     d\n   <dbl> <dbl> <dbl> <dbl>\n 1  2.64  1.71 4.18   1.26\n 2  2.83  1.42 0.178  1.89\n 3  2.56  1.06 2.21   1.80\n 4  4.43  2.71 0.995  1.38\n 5  3.62  1.84 1.64   2.16\n 6  3.05  1.99 0.872  2.69\n 7  2.22  3.96 0.937  3.36\n 8  1.18  2.65 1.33   1.16\n 9  2.23  2.70 2.77   1.34\n10  2.12  1.58 2.12   1.05\n\n# So why are maps powerful with functions. Answer: pipe stuff\ndf %>% \n  mutate(e = c(rep(\"dude\", 5), rep(\"sweet\", 5))) %>%\n  split(.$e) %>%\n  map(~ lm(a ~ b, data =.)) %>%\n  map(summary) %>%\n  map_dbl(\"r.squared\")\n\n\n      dude      sweet \n0.84405175 0.03733455 \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-18T09:36:32-05:00",
    "input_file": {}
  }
]
